{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddff350b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\gauri deoghare\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\gauri deoghare\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\gauri deoghare\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\gauri deoghare\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\gauri deoghare\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "TF-IDF Matrix:\n",
      "[[0.         0.09375901 0.         0.         0.         0.09375901\n",
      "  0.         0.05537558 0.         0.         0.         0.\n",
      "  0.18751802 0.09375901 0.         0.09375901 0.         0.\n",
      "  0.09375901 0.         0.09375901 0.07130614 0.07130614 0.\n",
      "  0.05537558 0.         0.         0.09375901 0.         0.\n",
      "  0.         0.09375901 0.         0.         0.         0.09375901\n",
      "  0.09375901 0.         0.09375901 0.         0.09375901 0.09375901\n",
      "  0.         0.09375901 0.18751802 0.         0.         0.09375901\n",
      "  0.09375901 0.         0.14261229 0.         0.07130614 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.09375901 0.         0.09375901 0.14261229\n",
      "  0.         0.09375901 0.05537558 0.16612674 0.07130614 0.\n",
      "  0.09375901 0.09375901 0.09375901 0.         0.09375901 0.09375901\n",
      "  0.09375901 0.         0.         0.         0.         0.07130614\n",
      "  0.09375901 0.09375901 0.07130614 0.         0.         0.\n",
      "  0.         0.09375901 0.09375901 0.22150233 0.         0.\n",
      "  0.07130614 0.07130614 0.09375901 0.         0.         0.\n",
      "  0.18751802 0.09375901 0.         0.         0.         0.\n",
      "  0.         0.         0.09375901 0.         0.         0.\n",
      "  0.07130614 0.         0.         0.09375901 0.         0.\n",
      "  0.         0.         0.         0.         0.09375901 0.09375901\n",
      "  0.         0.         0.09375901 0.         0.         0.\n",
      "  0.         0.09375901 0.         0.         0.09375901 0.\n",
      "  0.         0.49838023 0.         0.09375901 0.07130614 0.\n",
      "  0.         0.         0.18751802 0.         0.09375901 0.\n",
      "  0.09375901 0.         0.         0.05537558 0.         0.11075116\n",
      "  0.         0.         0.09375901 0.09375901 0.         0.09375901\n",
      "  0.         0.07130614 0.09375901 0.09375901 0.         0.09375901]\n",
      " [0.         0.         0.09386115 0.07138383 0.09386115 0.\n",
      "  0.         0.16630772 0.09386115 0.         0.09386115 0.07138383\n",
      "  0.         0.         0.         0.         0.09386115 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.05543591 0.37544461 0.09386115 0.         0.         0.\n",
      "  0.         0.         0.09386115 0.09386115 0.09386115 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.09386115 0.         0.         0.09386115 0.         0.\n",
      "  0.         0.09386115 0.07138383 0.09386115 0.         0.\n",
      "  0.         0.1877223  0.         0.         0.09386115 0.09386115\n",
      "  0.09386115 0.         0.         0.09386115 0.         0.21415148\n",
      "  0.         0.         0.22174363 0.11087182 0.07138383 0.\n",
      "  0.         0.         0.         0.09386115 0.         0.\n",
      "  0.         0.         0.09386115 0.09386115 0.         0.07138383\n",
      "  0.         0.         0.         0.         0.         0.09386115\n",
      "  0.         0.         0.         0.16630772 0.09386115 0.09386115\n",
      "  0.         0.07138383 0.         0.         0.07138383 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.09386115 0.         0.         0.         0.         0.09386115\n",
      "  0.14276765 0.09386115 0.         0.         0.09386115 0.09386115\n",
      "  0.09386115 0.09386115 0.09386115 0.         0.         0.\n",
      "  0.         0.         0.         0.09386115 0.         0.09386115\n",
      "  0.09386115 0.         0.         0.09386115 0.         0.\n",
      "  0.09386115 0.44348727 0.09386115 0.         0.         0.\n",
      "  0.         0.09386115 0.         0.09386115 0.         0.09386115\n",
      "  0.         0.09386115 0.         0.05543591 0.         0.05543591\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.09386115 0.14276765 0.         0.         0.09386115 0.        ]\n",
      " [0.09748847 0.         0.         0.07414249 0.         0.\n",
      "  0.09748847 0.23031304 0.         0.19497693 0.         0.07414249\n",
      "  0.         0.         0.09748847 0.         0.         0.09748847\n",
      "  0.         0.09748847 0.         0.07414249 0.07414249 0.09748847\n",
      "  0.05757826 0.         0.         0.         0.09748847 0.09748847\n",
      "  0.09748847 0.         0.         0.         0.         0.\n",
      "  0.         0.09748847 0.         0.09748847 0.         0.\n",
      "  0.         0.         0.         0.         0.09748847 0.\n",
      "  0.         0.         0.         0.         0.14828498 0.09748847\n",
      "  0.09748847 0.         0.09748847 0.09748847 0.         0.\n",
      "  0.         0.19497693 0.         0.         0.         0.\n",
      "  0.09748847 0.         0.17273478 0.11515652 0.         0.09748847\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.09748847 0.         0.         0.09748847 0.\n",
      "  0.         0.         0.07414249 0.09748847 0.09748847 0.\n",
      "  0.09748847 0.         0.         0.34546955 0.         0.\n",
      "  0.07414249 0.         0.         0.09748847 0.07414249 0.09748847\n",
      "  0.         0.         0.09748847 0.09748847 0.09748847 0.09748847\n",
      "  0.         0.09748847 0.         0.09748847 0.09748847 0.\n",
      "  0.         0.         0.09748847 0.         0.         0.\n",
      "  0.         0.         0.         0.09748847 0.         0.\n",
      "  0.09748847 0.09748847 0.         0.         0.09748847 0.\n",
      "  0.         0.         0.09748847 0.         0.         0.09748847\n",
      "  0.         0.34546955 0.         0.         0.07414249 0.09748847\n",
      "  0.09748847 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.09748847 0.05757826 0.09748847 0.05757826\n",
      "  0.09748847 0.2924654  0.         0.         0.09748847 0.\n",
      "  0.         0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "Feature Names (Vocabulary):\n",
      "['across' 'admire' 'adventure' 'along' 'also' 'an' 'ancient' 'and'\n",
      " 'anywhere' 'are' 'arizona' 'as' 'at' 'atmosphere' 'attractions'\n",
      " 'attracts' 'awe' 'badaling' 'beautiful' 'been' 'boat' 'breathtaking'\n",
      " 'built' 'by' 'can' 'canyon' 'carved' 'champs' 'china' 'chinas' 'chinese'\n",
      " 'climb' 'colorado' 'colorful' 'colors' 'creating' 'cruise' 'culture' 'de'\n",
      " 'deserts' 'different' 'dine' 'dramatically' 'each' 'eiffel' 'else'\n",
      " 'engineering' 'even' 'fair' 'famous' 'for' 'formations' 'from' 'fully'\n",
      " 'globally' 'grand' 'great' 'have' 'helicopter' 'heritage' 'hike'\n",
      " 'history' 'iconic' 'immense' 'impression' 'in' 'invasions' 'iron' 'is'\n",
      " 'it' 'its' 'kingdoms' 'leaves' 'levels' 'lights' 'located' 'magical'\n",
      " 'make' 'mars' 'marvel' 'massive' 'mesmerizing' 'miles' 'millions'\n",
      " 'monuments' 'more' 'most' 'mountains' 'mutianyu' 'natural' 'nature'\n",
      " 'nearby' 'night' 'of' 'offer' 'offers' 'one' 'or' 'originally' 'others'\n",
      " 'over' 'panoramic' 'paris' 'park' 'parts' 'perseverance' 'plains'\n",
      " 'protect' 'raft' 'reclaimed' 'restaurants' 'restored' 'rich' 'rim'\n",
      " 'river' 'rock' 'sections' 'seine' 'sense' 'shift' 'sights' 'site' 'size'\n",
      " 'some' 'sparkles' 'standing' 'stories' 'stretches' 'structure' 'stunning'\n",
      " 'such' 'sunrise' 'sunset' 'surroundings' 'symbol' 'take' 'tall' 'tells'\n",
      " 'that' 'the' 'this' 'thousands' 'to' 'top' 'tourist' 'tours' 'tower'\n",
      " 'unesco' 'unforgettable' 'unmatched' 'up' 'usa' 'various' 'views'\n",
      " 'visited' 'visitors' 'walk' 'wall' 'was' 'whether' 'while' 'with'\n",
      " 'wonder' 'world' 'worlds' 'year' 'years' 'you']\n"
     ]
    }
   ],
   "source": [
    "# Install scikit-learn (only if not already installed)\n",
    "!pip install scikit-learn\n",
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Function to read and preprocess text files\n",
    "def read_and_preprocess(files):\n",
    "    text_data = []\n",
    "    for file_path in files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            \n",
    "            # Clean the text (remove punctuation, special characters, numbers, extra spaces)\n",
    "            text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            text = text.lower()\n",
    "            text_data.append(text)\n",
    "    \n",
    "    return text_data\n",
    "\n",
    "# Function to apply TF-IDF encoding\n",
    "def apply_tfidf(text_data):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(text_data)\n",
    "    return tfidf_matrix, vectorizer.get_feature_names_out()\n",
    "\n",
    "# List of file paths (replace with your actual file names)\n",
    "files = ['place1.txt', 'place2.txt', 'place3.txt']\n",
    "\n",
    "# Step 1: Read and preprocess the text files\n",
    "text_data = read_and_preprocess(files)\n",
    "\n",
    "# Step 2: Apply TF-IDF\n",
    "tfidf_matrix, feature_names = apply_tfidf(text_data)\n",
    "\n",
    "# Step 3: Print the results\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_matrix.toarray())  # Convert sparse matrix to dense and print\n",
    "\n",
    "print(\"\\nFeature Names (Vocabulary):\")\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb103e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
