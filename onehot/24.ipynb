{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2a288ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoded Matrix:\n",
      "[[0 0 1 ... 1 0 0]\n",
      " [0 1 0 ... 1 1 0]\n",
      " [1 0 0 ... 0 0 1]]\n",
      "\n",
      "Feature Names (Vocabulary):\n",
      "['ability' 'accidents' 'accuracy' 'achieve' 'across' 'actions'\n",
      " 'additionally' 'addressed' 'advanced' 'advancements' 'agents' 'ai'\n",
      " 'algorithm' 'algorithmic' 'algorithms' 'also' 'amounts' 'an' 'analysis'\n",
      " 'and' 'another' 'applications' 'applied' 'architectures' 'are'\n",
      " 'artificial' 'as' 'assist' 'assistants' 'augmentation' 'automate'\n",
      " 'automatically' 'automating' 'automation' 'autonomous' 'availability'\n",
      " 'based' 'be' 'become' 'been' 'being' 'benefits' 'bert' 'bias' 'branch'\n",
      " 'branches' 'building' 'by' 'can' 'careful' 'challenge' 'challenges'\n",
      " 'chatbots' 'classification' 'cnns' 'common' 'commonly' 'complex'\n",
      " 'computational' 'computations' 'computer' 'computing' 'concerns'\n",
      " 'conditions' 'consist' 'continues' 'continuous' 'convolutional'\n",
      " 'creating' 'critical' 'crossvalidation' 'customer' 'data' 'dataset'\n",
      " 'datasets' 'deals' 'decision' 'decisionmaking' 'deep' 'designed'\n",
      " 'desired' 'despite' 'detecting' 'detection' 'develop' 'developed'\n",
      " 'diagnoses' 'diagnosing' 'diagnosis' 'different' 'direction' 'discovery'\n",
      " 'disease' 'diseases' 'displacement' 'driving' 'dropout' 'drug' 'due'\n",
      " 'effectiveness' 'efficiency' 'enables' 'engineering' 'enhanced'\n",
      " 'ensuring' 'entertainment' 'environment' 'error' 'essential' 'ethical'\n",
      " 'evaluate' 'evaluated' 'evaluation' 'evolve' 'experience' 'explicitly'\n",
      " 'feature' 'feedforward' 'field' 'fields' 'finance' 'find' 'flows'\n",
      " 'focused' 'for' 'forecasting' 'fraud' 'from' 'future' 'generalization'\n",
      " 'generalize' 'generate' 'generation' 'goal' 'gpt' 'gpus' 'great'\n",
      " 'groupings' 'hand' 'handle' 'hardware' 'has' 'have' 'healthcare' 'helps'\n",
      " 'holds' 'however' 'human' 'humanlevel' 'hyperparameters' 'image'\n",
      " 'imaging' 'impact' 'important' 'improve' 'improving' 'in' 'includes'\n",
      " 'including' 'increasingly' 'industries' 'information' 'innovation'\n",
      " 'input' 'instant' 'intelligence' 'interact' 'interconnected' 'interpret'\n",
      " 'into' 'involved' 'involves' 'is' 'issues' 'it' 'its' 'job' 'key' 'known'\n",
      " 'labeled' 'labels' 'language' 'large' 'layer' 'layers' 'learn' 'learning'\n",
      " 'learns' 'like' 'likely' 'linear' 'machine' 'machines' 'made' 'main'\n",
      " 'make' 'many' 'marketing' 'maximize' 'mean' 'medical' 'medicine'\n",
      " 'metrics' 'model' 'models' 'more' 'most' 'mse' 'much' 'multiple' 'must'\n",
      " 'natural' 'need' 'netflix' 'network' 'networks' 'neural' 'neurons' 'new'\n",
      " 'next' 'nlp' 'no' 'nodes' 'noise' 'normalization' 'number' 'object'\n",
      " 'occurs' 'of' 'often' 'on' 'one' 'ongoing' 'optimal' 'or' 'other'\n",
      " 'outcomes' 'outliers' 'output' 'overfitting' 'parallel' 'part'\n",
      " 'particularly' 'pass' 'patient' 'patterns' 'performance' 'personalized'\n",
      " 'platforms' 'play' 'plays' 'popular' 'positive' 'potential' 'power'\n",
      " 'powerful' 'precision' 'predict' 'predictions' 'preprocessing' 'prevent'\n",
      " 'privacy' 'problems' 'process' 'processing' 'profound' 'programmed'\n",
      " 'promising' 'provide' 'ranging' 'rate' 'realworld' 'recall' 'recent'\n",
      " 'recognition' 'recommendation' 'recurrent' 'reduce' 'reduces' 'reducing'\n",
      " 'regression' 'regularization' 'reinforcement' 'related' 'relationships'\n",
      " 'require' 'requirement' 'requires' 'research' 'resources' 'responsibly'\n",
      " 'results' 'revolutionized' 'reward' 'risk' 'rnns' 'role' 'rsquared'\n",
      " 'scaling' 'scans' 'score' 'sectors' 'segmentation' 'sentiment'\n",
      " 'sequencebased' 'sequential' 'service' 'services' 'shaping' 'significant'\n",
      " 'simulation' 'society' 'solving' 'specific' 'speech' 'split' 'spotify'\n",
      " 'squared' 'step' 'subfield' 'subfields' 'subset' 'such' 'supervised'\n",
      " 'support' 'surpassed' 'svms' 'system' 'systems' 'take' 'tasks'\n",
      " 'techniques' 'technologies' 'technology' 'text' 'that' 'the' 'there'\n",
      " 'think' 'this' 'through' 'timeseries' 'to' 'too' 'tpus' 'trading'\n",
      " 'traditionally' 'traffic' 'train' 'trained' 'training' 'transformer'\n",
      " 'translation' 'transportation' 'trees' 'tries' 'tumors' 'tuning' 'type'\n",
      " 'types' 'understand' 'understanding' 'unlabeled' 'unseen' 'unsupervised'\n",
      " 'use' 'used' 'users' 'uses' 'using' 'variables' 'various' 'vast' 'vector'\n",
      " 'vehicles' 'virtual' 'vision' 'ways' 'when' 'where' 'which' 'while'\n",
      " 'widely' 'widespread' 'will' 'with' 'within' 'without' 'years']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
   


    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Function to read and preprocess the text files\n",
    "def read_and_preprocess(files):\n",
    "    text_data = []\n",
    "    \n",
    "    for file_path in files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            \n",
    "            # Clean the text (remove punctuation, special characters, extra spaces)\n",
    "            text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove punctuation, numbers, etc.\n",
    "            text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with one\n",
    "            text = text.lower()  # Convert to lowercase\n",
    "            text_data.append(text)\n",
    "    \n",
    "    return text_data\n",
    "\n",
    "# Function to apply one-hot encoding (using CountVectorizer)\n",
    "def apply_one_hot_encoding(text_data):\n",
    "    vectorizer = CountVectorizer(binary=True)  # Set binary=True for one-hot encoding\n",
    "    one_hot_matrix = vectorizer.fit_transform(text_data)  # Apply one-hot encoding\n",
    "    return one_hot_matrix, vectorizer.get_feature_names_out()\n",
    "\n",
    "# List of file paths (Replace these paths with your actual file paths)\n",
    "files = ['file1.txt', 'file2.txt', 'file3.txt']  # Example file paths\n",
    "\n",
    "# Step 1: Read and preprocess the text files\n",
    "text_data = read_and_preprocess(files)\n",
    "\n",
    "# Step 2: Apply one-hot encoding\n",
    "one_hot_matrix, feature_names = apply_one_hot_encoding(text_data)\n",
    "\n",
    "# Step 3: Print the results\n",
    "print(\"One-Hot Encoded Matrix:\")\n",
    "print(one_hot_matrix.toarray())  # Convert sparse matrix to dense array and print\n",
    "\n",
    "print(\"\\nFeature Names (Vocabulary):\")\n",
    "print(feature_names)  # List of all unique words (features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae734933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
